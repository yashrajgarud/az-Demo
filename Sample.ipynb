{"cells":[{"cell_type":"code","source":["file_location = r\"/mnt/staging/people MAP.txt\"\nfile_type = \"csv\"\n\n\ninfer_schema = \"true\"\nfirst_row_is_header = \"true\"\ndelimiter = \",\"\n\n\ncurrentschema = spark.read.format(file_type) \\\n  .option(\"inferSchema\", infer_schema) \\\n  .option(\"header\", first_row_is_header) \\\n  .option(\"sep\", delimiter) \\\n  .load(file_location)\ncolumns  = [col.strip() for col in currentschema.columns]\ncurrentschema = currentschema.toDF(*columns)\n\ndisplay(currentschema)\n\ncurrentschema.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3da3f1ad-9aa9-4502-a8ef-81f6cf13c675"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[[123,"jack  ","qwe","dfg","m",99,2000,"cat","veg    "],[123,"jack  ","qwe","dfg","m",99,2000,"tiger","veg   "],[123,"jack  ","qwe","dfg","m",99,2000,"dog","veg     "],[124,"ram   ","asd","cvb","f",96,1000,"cat","non-veg     "],[125,"job   ","zxc","ert","m",55,3000,"mouse","non-veg   "],[126,"steve ","rty","tyu","m",85,4000,"dog","veg    "],[126,"steve ","rty","tyu","m",85,4000,"lion","veg    "],[126,"steve ","rty","tyu","m",85,4000,"camel","veg   "],[127,"mike  ","fgh","hjk","f",75,5000,"tiger","veg   "],[128,"sham  ","vbn","fds","f",69,6000,"elephant","veg"],[129,"suresh","iop","wer","m",65,8000,"girrafe","veg "]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"ID","type":"\"integer\"","metadata":"{}"},{"name":"FirstName","type":"\"string\"","metadata":"{}"},{"name":"MiddleName","type":"\"string\"","metadata":"{}"},{"name":"LastName","type":"\"string\"","metadata":"{}"},{"name":"Gender","type":"\"string\"","metadata":"{}"},{"name":"Birthyear","type":"\"integer\"","metadata":"{}"},{"name":"salary","type":"\"integer\"","metadata":"{}"},{"name":"Pet","type":"\"string\"","metadata":"{}"},{"name":"food","type":"\"string\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>ID</th><th>FirstName</th><th>MiddleName</th><th>LastName</th><th>Gender</th><th>Birthyear</th><th>salary</th><th>Pet</th><th>food</th></tr></thead><tbody><tr><td>123</td><td>jack  </td><td>qwe</td><td>dfg</td><td>m</td><td>99</td><td>2000</td><td>cat</td><td>veg    </td></tr><tr><td>123</td><td>jack  </td><td>qwe</td><td>dfg</td><td>m</td><td>99</td><td>2000</td><td>tiger</td><td>veg   </td></tr><tr><td>123</td><td>jack  </td><td>qwe</td><td>dfg</td><td>m</td><td>99</td><td>2000</td><td>dog</td><td>veg     </td></tr><tr><td>124</td><td>ram   </td><td>asd</td><td>cvb</td><td>f</td><td>96</td><td>1000</td><td>cat</td><td>non-veg     </td></tr><tr><td>125</td><td>job   </td><td>zxc</td><td>ert</td><td>m</td><td>55</td><td>3000</td><td>mouse</td><td>non-veg   </td></tr><tr><td>126</td><td>steve </td><td>rty</td><td>tyu</td><td>m</td><td>85</td><td>4000</td><td>dog</td><td>veg    </td></tr><tr><td>126</td><td>steve </td><td>rty</td><td>tyu</td><td>m</td><td>85</td><td>4000</td><td>lion</td><td>veg    </td></tr><tr><td>126</td><td>steve </td><td>rty</td><td>tyu</td><td>m</td><td>85</td><td>4000</td><td>camel</td><td>veg   </td></tr><tr><td>127</td><td>mike  </td><td>fgh</td><td>hjk</td><td>f</td><td>75</td><td>5000</td><td>tiger</td><td>veg   </td></tr><tr><td>128</td><td>sham  </td><td>vbn</td><td>fds</td><td>f</td><td>69</td><td>6000</td><td>elephant</td><td>veg</td></tr><tr><td>129</td><td>suresh</td><td>iop</td><td>wer</td><td>m</td><td>65</td><td>8000</td><td>girrafe</td><td>veg </td></tr></tbody></table></div>"]},"transient":null},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">root\n |-- ID: integer (nullable = true)\n |-- FirstName: string (nullable = true)\n |-- MiddleName: string (nullable = true)\n |-- LastName: string (nullable = true)\n |-- Gender: string (nullable = true)\n |-- Birthyear: integer (nullable = true)\n |-- salary: integer (nullable = true)\n |-- Pet: string (nullable = true)\n |-- food: string (nullable = true)\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">root\n-- ID: integer (nullable = true)\n-- FirstName: string (nullable = true)\n-- MiddleName: string (nullable = true)\n-- LastName: string (nullable = true)\n-- Gender: string (nullable = true)\n-- Birthyear: integer (nullable = true)\n-- salary: integer (nullable = true)\n-- Pet: string (nullable = true)\n-- food: string (nullable = true)\n\n</div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["import hashlib\n  def encrypt_value(ID):\n           sha_value = hashlib.sha256(ID.encode()).hexdigest()\n    return sha_value\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"25816524-8e32-49be-9104-37a94a3a718e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]},"transient":null},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"<div class=\"ansiout\"><span class=\"ansi-cyan-fg\">  File </span><span class=\"ansi-green-fg\">&#34;&lt;command-1256718231894482&gt;&#34;</span><span class=\"ansi-cyan-fg\">, line </span><span class=\"ansi-green-fg\">2</span>\n<span class=\"ansi-red-fg\">    def encrypt_value(ID):</span>\n    ^\n<span class=\"ansi-red-fg\">IndentationError</span><span class=\"ansi-red-fg\">:</span> unexpected indent\n</div>","errorSummary":"<span class=\"ansi-red-fg\">IndentationError</span><span class=\"ansi-red-fg\">:</span> unexpected indent","metadata":{},"errorTraceType":"html","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-cyan-fg\">  File </span><span class=\"ansi-green-fg\">&#34;&lt;command-1256718231894482&gt;&#34;</span><span class=\"ansi-cyan-fg\">, line </span><span class=\"ansi-green-fg\">2</span>\n<span class=\"ansi-red-fg\">    def encrypt_value(ID):</span>\n    ^\n<span class=\"ansi-red-fg\">IndentationError</span><span class=\"ansi-red-fg\">:</span> unexpected indent\n</div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import udf\nspark_udf = udf(encrypt_value, StringType())\ndata = data.withColumn('encrypted_value',spark_udf('ID'))\ndata.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4e07ddd8-791b-44fd-be07-21c12f719301"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]},"transient":null},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">NameError</span>                                 Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-1256718231894483&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      1</span> <span class=\"ansi-green-fg\">from</span> pyspark<span class=\"ansi-blue-fg\">.</span>sql<span class=\"ansi-blue-fg\">.</span>functions <span class=\"ansi-green-fg\">import</span> udf\n<span class=\"ansi-green-fg\">----&gt; 2</span><span class=\"ansi-red-fg\"> </span>spark_udf <span class=\"ansi-blue-fg\">=</span> udf<span class=\"ansi-blue-fg\">(</span>encrypt_value<span class=\"ansi-blue-fg\">,</span> StringType<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      3</span> data <span class=\"ansi-blue-fg\">=</span> data<span class=\"ansi-blue-fg\">.</span>withColumn<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;encrypted_value&#39;</span><span class=\"ansi-blue-fg\">,</span>spark_udf<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;ID&#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      4</span> data<span class=\"ansi-blue-fg\">.</span>show<span class=\"ansi-blue-fg\">(</span>truncate<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-green-fg\">False</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-red-fg\">NameError</span>: name &#39;encrypt_value&#39; is not defined</div>","errorSummary":"<span class=\"ansi-red-fg\">NameError</span>: name &#39;encrypt_value&#39; is not defined","metadata":{},"errorTraceType":"html","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">NameError</span>                                 Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-1256718231894483&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      1</span> <span class=\"ansi-green-fg\">from</span> pyspark<span class=\"ansi-blue-fg\">.</span>sql<span class=\"ansi-blue-fg\">.</span>functions <span class=\"ansi-green-fg\">import</span> udf\n<span class=\"ansi-green-fg\">----&gt; 2</span><span class=\"ansi-red-fg\"> </span>spark_udf <span class=\"ansi-blue-fg\">=</span> udf<span class=\"ansi-blue-fg\">(</span>encrypt_value<span class=\"ansi-blue-fg\">,</span> StringType<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      3</span> data <span class=\"ansi-blue-fg\">=</span> data<span class=\"ansi-blue-fg\">.</span>withColumn<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;encrypted_value&#39;</span><span class=\"ansi-blue-fg\">,</span>spark_udf<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;ID&#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      4</span> data<span class=\"ansi-blue-fg\">.</span>show<span class=\"ansi-blue-fg\">(</span>truncate<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-green-fg\">False</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-red-fg\">NameError</span>: name &#39;encrypt_value&#39; is not defined</div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["x = \"Python is \"\ny = \"awesome\"\nz =  x + y\nprint(z)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Rough","showTitle":true,"inputWidgets":{},"nuid":"d43c431a-69c8-4a6a-8f16-e3613333282e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Python is awesome\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Python is awesome\n</div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql import SparkSession\nspark=SparkSession.builder.appName(\"demo\").getOrCreate()\n\ncurrentschema = spark.read.format(\"csv\") \\\n  .option(\"inferSchema\", \"true\") \\\n  .option(\"header\", \"true\") \\\n  .option(\"delimiter\", \"|\") \\\n  .load(\"dbfs:/FileStore/Pipe_Seperated.txt\")\n\n\ndisplay(currentschema)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"29b061d1-2ac7-44ed-bdfb-7b22ea969d55"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[[123,"D1",1,"Y"],[345,"D2",2,"Y"]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"BRIGHT_CLAIM_ID","type":"\"integer\"","metadata":"{}"},{"name":"CONDITION_CODE","type":"\"string\"","metadata":"{}"},{"name":"CLAIM_SEQUENCE","type":"\"integer\"","metadata":"{}"},{"name":"CURRENT_IND","type":"\"string\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>BRIGHT_CLAIM_ID</th><th>CONDITION_CODE</th><th>CLAIM_SEQUENCE</th><th>CURRENT_IND</th></tr></thead><tbody><tr><td>123</td><td>D1</td><td>1</td><td>Y</td></tr><tr><td>345</td><td>D2</td><td>2</td><td>Y</td></tr></tbody></table></div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n# Create an instance of spark session\nspark_session = SparkSession.builder \\\n    .master('local[1]') \\\n    .appName('Example') \\\n    .getOrCreate()\ndf = spark_session.createDataFrame(\n    [\n        (1, '10-01-2020', '1.0', '100'),\n        (2, '14-02-2021', '2.0', '200'),\n        (3, '15-06-2019', '3.0', '300'),\n        (4, '12-12-2020', '4.0', '400'),\n        (5, '01-09-2019', '5.0', '500'),\n    ],\n    ['colA', 'colB', 'colC', 'colD']\n)\n\ndf.show()\ndf.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7497605a-de5f-4b71-ad41-869f6ec7a2e4"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+----+----------+----+----+\n|colA|      colB|colC|colD|\n+----+----------+----+----+\n|   1|10-01-2020| 1.0| 100|\n|   2|14-02-2021| 2.0| 200|\n|   3|15-06-2019| 3.0| 300|\n|   4|12-12-2020| 4.0| 400|\n|   5|01-09-2019| 5.0| 500|\n+----+----------+----+----+\n\nroot\n |-- colA: long (nullable = true)\n |-- colB: string (nullable = true)\n |-- colC: string (nullable = true)\n |-- colD: string (nullable = true)\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+----------+----+----+\ncolA|      colB|colC|colD|\n+----+----------+----+----+\n   1|10-01-2020| 1.0| 100|\n   2|14-02-2021| 2.0| 200|\n   3|15-06-2019| 3.0| 300|\n   4|12-12-2020| 4.0| 400|\n   5|01-09-2019| 5.0| 500|\n+----+----------+----+----+\n\nroot\n-- colA: long (nullable = true)\n-- colB: string (nullable = true)\n-- colC: string (nullable = true)\n-- colD: string (nullable = true)\n\n</div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["for i in df:\n    # display\n     print([\"colA\"],[\"colB\"])"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ce389522-84de-4331-821f-322c9e8dcae1"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">[&#39;colA&#39;] [&#39;colB&#39;]\n[&#39;colA&#39;] [&#39;colB&#39;]\n[&#39;colA&#39;] [&#39;colB&#39;]\n[&#39;colA&#39;] [&#39;colB&#39;]\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">[&#39;colA&#39;] [&#39;colB&#39;]\n[&#39;colA&#39;] [&#39;colB&#39;]\n[&#39;colA&#39;] [&#39;colB&#39;]\n[&#39;colA&#39;] [&#39;colB&#39;]\n</div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n# Create an instance of spark session\nspark_session = SparkSession.builder \\\n    .master('local[1]') \\\n    .appName('Example') \\\n    .getOrCreate()\ndf1 = spark_session.createDataFrame(\n    [\n        ('10-01-2020', '1.0', '100'),\n        ('14-02-2021', '2.0', '200'),\n        ('15-06-2019', '3.0', '300'),\n        ('12-12-2020', '4.0', '400'),\n        ('01-09-2019', '5.0', '500'),\n    ],\n    ['colB', 'first', 'second']\n)\n\ndf1.show()\ndf1.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a9254a6e-87ac-4944-a30d-2471ed8c6ca7"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+----------+-----+------+\n|      colB|first|second|\n+----------+-----+------+\n|10-01-2020|  1.0|   100|\n|14-02-2021|  2.0|   200|\n|15-06-2019|  3.0|   300|\n|12-12-2020|  4.0|   400|\n|01-09-2019|  5.0|   500|\n+----------+-----+------+\n\nroot\n |-- colB: string (nullable = true)\n |-- first: string (nullable = true)\n |-- second: string (nullable = true)\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----------+-----+------+\n      colB|first|second|\n+----------+-----+------+\n10-01-2020|  1.0|   100|\n14-02-2021|  2.0|   200|\n15-06-2019|  3.0|   300|\n12-12-2020|  4.0|   400|\n01-09-2019|  5.0|   500|\n+----------+-----+------+\n\nroot\n-- colB: string (nullable = true)\n-- first: string (nullable = true)\n-- second: string (nullable = true)\n\n</div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["df_outer = df.join(df1, on=['colB'], how='outer')\ndf_outer.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"88de6fb7-f00c-41fc-9ab9-6b56c509d455"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+----------+----+----+----+-----+------+\n|      colB|colA|colC|colD|first|second|\n+----------+----+----+----+-----+------+\n|01-09-2019|   5| 5.0| 500|  5.0|   500|\n|10-01-2020|   1| 1.0| 100|  1.0|   100|\n|12-12-2020|   4| 4.0| 400|  4.0|   400|\n|14-02-2021|   2| 2.0| 200|  2.0|   200|\n|15-06-2019|   3| 3.0| 300|  3.0|   300|\n+----------+----+----+----+-----+------+\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----------+----+----+----+-----+------+\n      colB|colA|colC|colD|first|second|\n+----------+----+----+----+-----+------+\n01-09-2019|   5| 5.0| 500|  5.0|   500|\n10-01-2020|   1| 1.0| 100|  1.0|   100|\n12-12-2020|   4| 4.0| 400|  4.0|   400|\n14-02-2021|   2| 2.0| 200|  2.0|   200|\n15-06-2019|   3| 3.0| 300|  3.0|   300|\n+----------+----+----+----+-----+------+\n\n</div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["def autoIncrement():\n    global rec\n    if (rec == 0) : rec = 1 \n    else : rec = rec + 1\n    return int(rec)\n\nrec = 0"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bbb1e8ca-86f8-4459-bf5c-6f14474918d5"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["df_outer.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"132c1ad0-eea3-4428-8f64-44656a9d92db"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+----------+----+----+----+-----+------+---+\n|      colB|colA|colC|colD|first|second|id2|\n+----------+----+----+----+-----+------+---+\n|01-09-2019|   5| 5.0| 500|  5.0|   500|  1|\n|10-01-2020|   1| 1.0| 100|  1.0|   100|  2|\n|12-12-2020|   4| 4.0| 400|  4.0|   400|  3|\n|14-02-2021|   2| 2.0| 200|  2.0|   200|  4|\n|15-06-2019|   3| 3.0| 300|  3.0|   300|  5|\n+----------+----+----+----+-----+------+---+\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----------+----+----+----+-----+------+---+\n      colB|colA|colC|colD|first|second|id2|\n+----------+----+----+----+-----+------+---+\n01-09-2019|   5| 5.0| 500|  5.0|   500|  1|\n10-01-2020|   1| 1.0| 100|  1.0|   100|  2|\n12-12-2020|   4| 4.0| 400|  4.0|   400|  3|\n14-02-2021|   2| 2.0| 200|  2.0|   200|  4|\n15-06-2019|   3| 3.0| 300|  3.0|   300|  5|\n+----------+----+----+----+-----+------+---+\n\n</div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["# Function to check the value of \ndef assign_rec():\n    global rec\n    rec =0\n    try:\n        rec = df_outer.collect()[-1]['id2']\n        return int(rec)\n    except:\n        rec = 0\n        return int(rec)\n\ndef autoIncrement():\n    global rec\n    if (rec == 0) : rec = 1 \n    else : rec = rec + 1\n    return int(rec)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"581c2aad-9924-48f2-b6d0-dffbb6ddbfe3"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["# Function to check the value of \ndef assign_rec(df, column):\n    global rec\n    rec =0\n    try:\n        rec = df.collect()[-1][column]\n        return int(rec)\n    except:\n        rec = 0\n        return int(rec)\n\ndef autoIncrement():\n    global rec\n    if (rec == 0) : rec = 1 \n    else : rec = rec + 1\n    return int(rec)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"95132d01-2045-44b5-a746-c7080aaf24ce"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import *\nfrom pyspark.sql.types import *\n\nautoIncrementUDF = udf(autoIncrement,  IntegerType())\nassign_rec(df_outer,'Cycle_Sk')\ndf_outer = df_outer.withColumn(\"Cycle_Sk\", autoIncrementUDF())\n\ndf_outer.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4e845536-cccd-4991-b725-271c46520b84"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+----------+----+----+----+-----+------+---+--------+\n|      colB|colA|colC|colD|first|second|id2|Cycle_Sk|\n+----------+----+----+----+-----+------+---+--------+\n|01-09-2019|   5| 5.0| 500|  5.0|   500| 16|      11|\n|10-01-2020|   1| 1.0| 100|  1.0|   100| 17|      12|\n|12-12-2020|   4| 4.0| 400|  4.0|   400| 18|      13|\n|14-02-2021|   2| 2.0| 200|  2.0|   200| 19|      14|\n|15-06-2019|   3| 3.0| 300|  3.0|   300| 20|      15|\n+----------+----+----+----+-----+------+---+--------+\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----------+----+----+----+-----+------+---+--------+\n      colB|colA|colC|colD|first|second|id2|Cycle_Sk|\n+----------+----+----+----+-----+------+---+--------+\n01-09-2019|   5| 5.0| 500|  5.0|   500| 16|      11|\n10-01-2020|   1| 1.0| 100|  1.0|   100| 17|      12|\n12-12-2020|   4| 4.0| 400|  4.0|   400| 18|      13|\n14-02-2021|   2| 2.0| 200|  2.0|   200| 19|      14|\n15-06-2019|   3| 3.0| 300|  3.0|   300| 20|      15|\n+----------+----+----+----+-----+------+---+--------+\n\n</div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["df_outer1.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"da62ef5d-fc0a-4a51-9e79-b505de2ce210"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+----------+----+----+----+-----+------+---+\n|      colB|colA|colC|colD|first|second|id2|\n+----------+----+----+----+-----+------+---+\n|01-09-2019|   5| 5.0| 500|  5.0|   500|  1|\n|10-01-2020|   1| 1.0| 100|  1.0|   100|  2|\n|12-12-2020|   4| 4.0| 400|  4.0|   400|  3|\n|14-02-2021|   2| 2.0| 200|  2.0|   200|  4|\n|15-06-2019|   3| 3.0| 300|  3.0|   300|  5|\n+----------+----+----+----+-----+------+---+\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----------+----+----+----+-----+------+---+\n      colB|colA|colC|colD|first|second|id2|\n+----------+----+----+----+-----+------+---+\n01-09-2019|   5| 5.0| 500|  5.0|   500|  1|\n10-01-2020|   1| 1.0| 100|  1.0|   100|  2|\n12-12-2020|   4| 4.0| 400|  4.0|   400|  3|\n14-02-2021|   2| 2.0| 200|  2.0|   200|  4|\n15-06-2019|   3| 3.0| 300|  3.0|   300|  5|\n+----------+----+----+----+-----+------+---+\n\n</div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["#Find the last record of the dataframe column id2\nlast_record = df_outer1.collect()[-1]['id2']\nprint(last_record)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9a52621f-ac58-4425-a974-1da7c0d4f577"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">10\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">10\n</div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["def autoIncrement(df,column):\n    #global rec\n    rec = df.collect()[-1][column]\n    if (rec == 0) : rec = 1 \n    else : rec = rec + 1\n    return int(rec)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1d2fbe29-4bd2-4335-8c8e-777fa6a81967"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["# Check all columns starts with col in a dataframe\nll = []\nfor a in df.columns:\n    if a.startswith('col'):\n        ll.append(a)\ndf2 = df[ll]\n\ndf2.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8b02082f-c78a-4203-a8e5-681935b2218f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+----+----------+----+----+\n|colA|      colB|colC|colD|\n+----+----------+----+----+\n|   1|10-01-2020| 1.0| 100|\n|   2|14-02-2021| 2.0| 200|\n|   3|15-06-2019| 3.0| 300|\n|   4|12-12-2020| 4.0| 400|\n|   5|01-09-2019| 5.0| 500|\n+----+----------+----+----+\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+----------+----+----+\ncolA|      colB|colC|colD|\n+----+----------+----+----+\n   1|10-01-2020| 1.0| 100|\n   2|14-02-2021| 2.0| 200|\n   3|15-06-2019| 3.0| 300|\n   4|12-12-2020| 4.0| 400|\n   5|01-09-2019| 5.0| 500|\n+----+----------+----+----+\n\n</div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["from datetime import datetime\nfrom pyspark.sql.functions import col, udf\nfrom pyspark.sql.types import DoubleType, IntegerType, DateType\n\n# UDF to process the date column\nfunc = udf(lambda x: datetime.strptime(x, '%d-%m-%Y'), DateType())\ndf = df \\\n    .withColumn('colB', func(col('colB'))) \\\n    .withColumn('colC', col('colC').cast(DoubleType())) \\\n    .withColumn('colD', col('colD').cast(IntegerType()))\ndf.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"866bd6d1-8363-4ab2-9a1a-abfacc4158e0"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+----+----------+----+----+\n|colA|      colB|colC|colD|\n+----+----------+----+----+\n|   1|2020-01-10| 1.0| 100|\n|   2|2021-02-14| 2.0| 200|\n|   3|2019-06-15| 3.0| 300|\n|   4|2020-12-12| 4.0| 400|\n|   5|2019-09-01| 5.0| 500|\n+----+----------+----+----+\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+----------+----+----+\ncolA|      colB|colC|colD|\n+----+----------+----+----+\n   1|2020-01-10| 1.0| 100|\n   2|2021-02-14| 2.0| 200|\n   3|2019-06-15| 3.0| 300|\n   4|2020-12-12| 4.0| 400|\n   5|2019-09-01| 5.0| 500|\n+----+----------+----+----+\n\n</div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["from datetime import datetime\nfrom pyspark.sql.functions import col,udf\nfrom pyspark.sql.types import DateType\n\n\nrdd = sc.parallelize(['20161231', '20140102', '20151201', '20161124'])\ndf1 = sqlContext.createDataFrame(rdd, ['old_col'])\n\n# UDF to convert string to date\nfunc =  udf (lambda x: datetime.strptime(x, '%Y%m%d'), DateType())\n\ndf = df1.withColumn('new_col', date_format(func(col('old_col')), 'MM-dd-yyy'))\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4d21a914-e4ed-4880-986e-0a8acbb63cac"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]},"transient":null},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">TypeError</span>                                 Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-101278144649926&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      5</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">      6</span> rdd <span class=\"ansi-blue-fg\">=</span> sc<span class=\"ansi-blue-fg\">.</span>parallelize<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-blue-fg\">&#39;20161231&#39;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">&#39;20140102&#39;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">&#39;20151201&#39;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">&#39;20161124&#39;</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">----&gt; 7</span><span class=\"ansi-red-fg\"> </span>df1 <span class=\"ansi-blue-fg\">=</span> sqlContext<span class=\"ansi-blue-fg\">.</span>createDataFrame<span class=\"ansi-blue-fg\">(</span>rdd<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">[</span><span class=\"ansi-blue-fg\">&#39;old_col&#39;</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      8</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">      9</span> <span class=\"ansi-red-fg\"># UDF to convert string to date</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/context.py</span> in <span class=\"ansi-cyan-fg\">createDataFrame</span><span class=\"ansi-blue-fg\">(self, data, schema, samplingRatio, verifySchema)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    384</span>         Py4JJavaError<span class=\"ansi-blue-fg\">:</span> <span class=\"ansi-blue-fg\">...</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    385</span>         &#34;&#34;&#34;\n<span class=\"ansi-green-fg\">--&gt; 386</span><span class=\"ansi-red-fg\">         </span><span class=\"ansi-green-fg\">return</span> self<span class=\"ansi-blue-fg\">.</span>sparkSession<span class=\"ansi-blue-fg\">.</span>createDataFrame<span class=\"ansi-blue-fg\">(</span>data<span class=\"ansi-blue-fg\">,</span> schema<span class=\"ansi-blue-fg\">,</span> samplingRatio<span class=\"ansi-blue-fg\">,</span> verifySchema<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    387</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">    388</span>     <span class=\"ansi-green-fg\">def</span> registerDataFrameAsTable<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">,</span> df<span class=\"ansi-blue-fg\">,</span> tableName<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/session.py</span> in <span class=\"ansi-cyan-fg\">createDataFrame</span><span class=\"ansi-blue-fg\">(self, data, schema, samplingRatio, verifySchema)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    720</span>             return super(SparkSession, self).createDataFrame(\n<span class=\"ansi-green-intense-fg ansi-bold\">    721</span>                 data, schema, samplingRatio, verifySchema)\n<span class=\"ansi-green-fg\">--&gt; 722</span><span class=\"ansi-red-fg\">         </span><span class=\"ansi-green-fg\">return</span> self<span class=\"ansi-blue-fg\">.</span>_create_dataframe<span class=\"ansi-blue-fg\">(</span>data<span class=\"ansi-blue-fg\">,</span> schema<span class=\"ansi-blue-fg\">,</span> samplingRatio<span class=\"ansi-blue-fg\">,</span> verifySchema<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    723</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">    724</span>     <span class=\"ansi-green-fg\">def</span> _create_dataframe<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">,</span> data<span class=\"ansi-blue-fg\">,</span> schema<span class=\"ansi-blue-fg\">,</span> samplingRatio<span class=\"ansi-blue-fg\">,</span> verifySchema<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/session.py</span> in <span class=\"ansi-cyan-fg\">_create_dataframe</span><span class=\"ansi-blue-fg\">(self, data, schema, samplingRatio, verifySchema)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    750</span>         <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    751</span>             <span class=\"ansi-green-fg\">if</span> isinstance<span class=\"ansi-blue-fg\">(</span>data<span class=\"ansi-blue-fg\">,</span> RDD<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 752</span><span class=\"ansi-red-fg\">                 </span>rdd<span class=\"ansi-blue-fg\">,</span> schema <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>_createFromRDD<span class=\"ansi-blue-fg\">(</span>data<span class=\"ansi-blue-fg\">.</span>map<span class=\"ansi-blue-fg\">(</span>prepare<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> schema<span class=\"ansi-blue-fg\">,</span> samplingRatio<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    753</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    754</span>                 rdd<span class=\"ansi-blue-fg\">,</span> schema <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>_createFromLocal<span class=\"ansi-blue-fg\">(</span>map<span class=\"ansi-blue-fg\">(</span>prepare<span class=\"ansi-blue-fg\">,</span> data<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> schema<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/session.py</span> in <span class=\"ansi-cyan-fg\">_createFromRDD</span><span class=\"ansi-blue-fg\">(self, rdd, schema, samplingRatio)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    485</span>         &#34;&#34;&#34;\n<span class=\"ansi-green-intense-fg ansi-bold\">    486</span>         <span class=\"ansi-green-fg\">if</span> schema <span class=\"ansi-green-fg\">is</span> <span class=\"ansi-green-fg\">None</span> <span class=\"ansi-green-fg\">or</span> isinstance<span class=\"ansi-blue-fg\">(</span>schema<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">(</span>list<span class=\"ansi-blue-fg\">,</span> tuple<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 487</span><span class=\"ansi-red-fg\">             </span>struct <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>_inferSchema<span class=\"ansi-blue-fg\">(</span>rdd<span class=\"ansi-blue-fg\">,</span> samplingRatio<span class=\"ansi-blue-fg\">,</span> names<span class=\"ansi-blue-fg\">=</span>schema<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    488</span>             converter <span class=\"ansi-blue-fg\">=</span> _create_converter<span class=\"ansi-blue-fg\">(</span>struct<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    489</span>             rdd <span class=\"ansi-blue-fg\">=</span> rdd<span class=\"ansi-blue-fg\">.</span>map<span class=\"ansi-blue-fg\">(</span>converter<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/session.py</span> in <span class=\"ansi-cyan-fg\">_inferSchema</span><span class=\"ansi-blue-fg\">(self, rdd, samplingRatio, names)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    465</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">    466</span>         <span class=\"ansi-green-fg\">if</span> samplingRatio <span class=\"ansi-green-fg\">is</span> <span class=\"ansi-green-fg\">None</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 467</span><span class=\"ansi-red-fg\">             </span>schema <span class=\"ansi-blue-fg\">=</span> _infer_schema<span class=\"ansi-blue-fg\">(</span>first<span class=\"ansi-blue-fg\">,</span> names<span class=\"ansi-blue-fg\">=</span>names<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    468</span>             <span class=\"ansi-green-fg\">if</span> _has_nulltype<span class=\"ansi-blue-fg\">(</span>schema<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    469</span>                 <span class=\"ansi-green-fg\">for</span> row <span class=\"ansi-green-fg\">in</span> rdd<span class=\"ansi-blue-fg\">.</span>take<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-cyan-fg\">100</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">:</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/types.py</span> in <span class=\"ansi-cyan-fg\">_infer_schema</span><span class=\"ansi-blue-fg\">(row, names)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1063</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1064</span>     <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">-&gt; 1065</span><span class=\"ansi-red-fg\">         </span><span class=\"ansi-green-fg\">raise</span> TypeError<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;Can not infer schema for type: %s&#34;</span> <span class=\"ansi-blue-fg\">%</span> type<span class=\"ansi-blue-fg\">(</span>row<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1066</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1067</span>     fields <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-blue-fg\">[</span><span class=\"ansi-blue-fg\">]</span>\n\n<span class=\"ansi-red-fg\">TypeError</span>: Can not infer schema for type: &lt;class &#39;str&#39;&gt;</div>","errorSummary":"<span class=\"ansi-red-fg\">TypeError</span>: Can not infer schema for type: &lt;class &#39;str&#39;&gt;","metadata":{},"errorTraceType":"html","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">TypeError</span>                                 Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-101278144649926&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      5</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">      6</span> rdd <span class=\"ansi-blue-fg\">=</span> sc<span class=\"ansi-blue-fg\">.</span>parallelize<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-blue-fg\">&#39;20161231&#39;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">&#39;20140102&#39;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">&#39;20151201&#39;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">&#39;20161124&#39;</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">----&gt; 7</span><span class=\"ansi-red-fg\"> </span>df1 <span class=\"ansi-blue-fg\">=</span> sqlContext<span class=\"ansi-blue-fg\">.</span>createDataFrame<span class=\"ansi-blue-fg\">(</span>rdd<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">[</span><span class=\"ansi-blue-fg\">&#39;old_col&#39;</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      8</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">      9</span> <span class=\"ansi-red-fg\"># UDF to convert string to date</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/context.py</span> in <span class=\"ansi-cyan-fg\">createDataFrame</span><span class=\"ansi-blue-fg\">(self, data, schema, samplingRatio, verifySchema)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    384</span>         Py4JJavaError<span class=\"ansi-blue-fg\">:</span> <span class=\"ansi-blue-fg\">...</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    385</span>         &#34;&#34;&#34;\n<span class=\"ansi-green-fg\">--&gt; 386</span><span class=\"ansi-red-fg\">         </span><span class=\"ansi-green-fg\">return</span> self<span class=\"ansi-blue-fg\">.</span>sparkSession<span class=\"ansi-blue-fg\">.</span>createDataFrame<span class=\"ansi-blue-fg\">(</span>data<span class=\"ansi-blue-fg\">,</span> schema<span class=\"ansi-blue-fg\">,</span> samplingRatio<span class=\"ansi-blue-fg\">,</span> verifySchema<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    387</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">    388</span>     <span class=\"ansi-green-fg\">def</span> registerDataFrameAsTable<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">,</span> df<span class=\"ansi-blue-fg\">,</span> tableName<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/session.py</span> in <span class=\"ansi-cyan-fg\">createDataFrame</span><span class=\"ansi-blue-fg\">(self, data, schema, samplingRatio, verifySchema)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    720</span>             return super(SparkSession, self).createDataFrame(\n<span class=\"ansi-green-intense-fg ansi-bold\">    721</span>                 data, schema, samplingRatio, verifySchema)\n<span class=\"ansi-green-fg\">--&gt; 722</span><span class=\"ansi-red-fg\">         </span><span class=\"ansi-green-fg\">return</span> self<span class=\"ansi-blue-fg\">.</span>_create_dataframe<span class=\"ansi-blue-fg\">(</span>data<span class=\"ansi-blue-fg\">,</span> schema<span class=\"ansi-blue-fg\">,</span> samplingRatio<span class=\"ansi-blue-fg\">,</span> verifySchema<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    723</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">    724</span>     <span class=\"ansi-green-fg\">def</span> _create_dataframe<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">,</span> data<span class=\"ansi-blue-fg\">,</span> schema<span class=\"ansi-blue-fg\">,</span> samplingRatio<span class=\"ansi-blue-fg\">,</span> verifySchema<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/session.py</span> in <span class=\"ansi-cyan-fg\">_create_dataframe</span><span class=\"ansi-blue-fg\">(self, data, schema, samplingRatio, verifySchema)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    750</span>         <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    751</span>             <span class=\"ansi-green-fg\">if</span> isinstance<span class=\"ansi-blue-fg\">(</span>data<span class=\"ansi-blue-fg\">,</span> RDD<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 752</span><span class=\"ansi-red-fg\">                 </span>rdd<span class=\"ansi-blue-fg\">,</span> schema <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>_createFromRDD<span class=\"ansi-blue-fg\">(</span>data<span class=\"ansi-blue-fg\">.</span>map<span class=\"ansi-blue-fg\">(</span>prepare<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> schema<span class=\"ansi-blue-fg\">,</span> samplingRatio<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    753</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    754</span>                 rdd<span class=\"ansi-blue-fg\">,</span> schema <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>_createFromLocal<span class=\"ansi-blue-fg\">(</span>map<span class=\"ansi-blue-fg\">(</span>prepare<span class=\"ansi-blue-fg\">,</span> data<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> schema<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/session.py</span> in <span class=\"ansi-cyan-fg\">_createFromRDD</span><span class=\"ansi-blue-fg\">(self, rdd, schema, samplingRatio)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    485</span>         &#34;&#34;&#34;\n<span class=\"ansi-green-intense-fg ansi-bold\">    486</span>         <span class=\"ansi-green-fg\">if</span> schema <span class=\"ansi-green-fg\">is</span> <span class=\"ansi-green-fg\">None</span> <span class=\"ansi-green-fg\">or</span> isinstance<span class=\"ansi-blue-fg\">(</span>schema<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">(</span>list<span class=\"ansi-blue-fg\">,</span> tuple<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 487</span><span class=\"ansi-red-fg\">             </span>struct <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>_inferSchema<span class=\"ansi-blue-fg\">(</span>rdd<span class=\"ansi-blue-fg\">,</span> samplingRatio<span class=\"ansi-blue-fg\">,</span> names<span class=\"ansi-blue-fg\">=</span>schema<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    488</span>             converter <span class=\"ansi-blue-fg\">=</span> _create_converter<span class=\"ansi-blue-fg\">(</span>struct<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    489</span>             rdd <span class=\"ansi-blue-fg\">=</span> rdd<span class=\"ansi-blue-fg\">.</span>map<span class=\"ansi-blue-fg\">(</span>converter<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/session.py</span> in <span class=\"ansi-cyan-fg\">_inferSchema</span><span class=\"ansi-blue-fg\">(self, rdd, samplingRatio, names)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    465</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">    466</span>         <span class=\"ansi-green-fg\">if</span> samplingRatio <span class=\"ansi-green-fg\">is</span> <span class=\"ansi-green-fg\">None</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 467</span><span class=\"ansi-red-fg\">             </span>schema <span class=\"ansi-blue-fg\">=</span> _infer_schema<span class=\"ansi-blue-fg\">(</span>first<span class=\"ansi-blue-fg\">,</span> names<span class=\"ansi-blue-fg\">=</span>names<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    468</span>             <span class=\"ansi-green-fg\">if</span> _has_nulltype<span class=\"ansi-blue-fg\">(</span>schema<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    469</span>                 <span class=\"ansi-green-fg\">for</span> row <span class=\"ansi-green-fg\">in</span> rdd<span class=\"ansi-blue-fg\">.</span>take<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-cyan-fg\">100</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">:</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/types.py</span> in <span class=\"ansi-cyan-fg\">_infer_schema</span><span class=\"ansi-blue-fg\">(row, names)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1063</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1064</span>     <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">-&gt; 1065</span><span class=\"ansi-red-fg\">         </span><span class=\"ansi-green-fg\">raise</span> TypeError<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;Can not infer schema for type: %s&#34;</span> <span class=\"ansi-blue-fg\">%</span> type<span class=\"ansi-blue-fg\">(</span>row<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1066</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1067</span>     fields <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-blue-fg\">[</span><span class=\"ansi-blue-fg\">]</span>\n\n<span class=\"ansi-red-fg\">TypeError</span>: Can not infer schema for type: &lt;class &#39;str&#39;&gt;</div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import to_date\n\nspark = SparkSession.builder.appName(\"Python Spark SQL basic example\")\\\n    .config(\"spark.some.config.option\", \"some-value\").getOrCreate()\n\n\ndf = spark.createDataFrame([('20190622',)], ['t'])\n#df1 = df.select(to_date(df.t, 'yyyy-MM-dd').alias('dt'))\n#df1.show()\ndf.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"192b2dfa-3035-4b63-a6fd-7cd392f56e89"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+--------+\n|       t|\n+--------+\n|20190622|\n+--------+\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------+\n       t|\n+--------+\n20190622|\n+--------+\n\n</div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["import pyspark.sql.functions as F\nfrom pyspark.sql.functions import to_date\n\ndf2 = df.withColumn(\n    \"t\", \n    F.to_date(F.col(\"t\").cast(\"string\"), \"yyyyMMdd\")\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"515c0e22-3205-44d5-b345-45596be7e188"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["df2.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c1436f41-cf0c-4e20-b8a3-81f18e8dea26"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+----------+\n|         t|\n+----------+\n|2019-06-22|\n+----------+\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----------+\n         t|\n+----------+\n2019-06-22|\n+----------+\n\n</div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["\nfrom pyspark.sql.functions import *\ndf=spark.createDataFrame([[\"02-03-2013\"],[\"05-06-2023\"]],[\"input\"])\ndf.select(col(\"input\"),date_format(col(\"input\"),\"MM-dd-yyyy\").alias(\"date\")) \\\n  .show()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2f183943-a2c8-46b9-b370-2e3914a1ab93"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+----------+----+\n|     input|date|\n+----------+----+\n|02-03-2013|null|\n|05-06-2023|null|\n+----------+----+\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----------+----+\n     input|date|\n+----------+----+\n02-03-2013|null|\n05-06-2023|null|\n+----------+----+\n\n</div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import to_date \ndf = spark.createDataFrame([(\"Mar 25, 1991\",), (\"May 1, 2020\",)],['date_str'])\ndf.select(to_date(df.date_str, 'MMM d, yyyy').alias('dt')).collect()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5a3e1ed8-81fc-4ec7-9169-7be11a222c00"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[24]: [Row(dt=datetime.date(1991, 3, 25)), Row(dt=datetime.date(2020, 5, 1))]</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[24]: [Row(dt=datetime.date(1991, 3, 25)), Row(dt=datetime.date(2020, 5, 1))]</div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["a_string = \"\"\n\ntry:\n    an_integer = int(a_string)\n    print(an_integer)\nexcept ValueError:\n    print(\"Catch Error\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7de45b06-f4bd-468b-a7d5-afb8c1abb6d2"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Catch Error\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Catch Error\n</div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import to_date\ndf111 = spark.createDataFrame([(\"20190311\",), (\"20190426\",), (\" \",)], ['date_str'])\ndisplay(df111)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9e6d3a6d-3606-4000-941d-1383484dce18"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["20190311"],["20190426"],[" "]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"date_str","type":"\"string\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>date_str</th></tr></thead><tbody><tr><td>20190311</td></tr><tr><td>20190426</td></tr><tr><td> </td></tr></tbody></table></div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["def castToInt(df,column):\n        try:\n               return df[column].cast(IntegerType())\n        except ValueError:\n              return 0\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a334c272-bcc2-4a75-b557-a84c86a23b15"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.types import *\ndf= df111.withColumn(\"newcol\", castToInt(df111,'date_str')).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5cad4358-9962-4353-82cb-ffe67be8dd88"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+--------+--------+\n|date_str|  newcol|\n+--------+--------+\n|20190311|20190311|\n|20190426|20190426|\n|        |    null|\n+--------+--------+\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------+--------+\ndate_str|  newcol|\n+--------+--------+\n20190311|20190311|\n20190426|20190426|\n        |    null|\n+--------+--------+\n\n</div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import col\ndf= df111.withColumn(\"castedcol\", [df111.dtypes]).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0283e87c-aba1-42ae-8bb2-e47604c9efb0"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]},"transient":null},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">AssertionError</span>                            Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-579747323490749&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      1</span> <span class=\"ansi-green-fg\">from</span> pyspark<span class=\"ansi-blue-fg\">.</span>sql<span class=\"ansi-blue-fg\">.</span>functions <span class=\"ansi-green-fg\">import</span> col\n<span class=\"ansi-green-fg\">----&gt; 2</span><span class=\"ansi-red-fg\"> </span>df<span class=\"ansi-blue-fg\">=</span> df111<span class=\"ansi-blue-fg\">.</span>withColumn<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;castedcol&#34;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">[</span>df111<span class=\"ansi-blue-fg\">.</span>dtypes<span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>show<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/dataframe.py</span> in <span class=\"ansi-cyan-fg\">withColumn</span><span class=\"ansi-blue-fg\">(self, colName, col)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   2475</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   2476</span>         &#34;&#34;&#34;\n<span class=\"ansi-green-fg\">-&gt; 2477</span><span class=\"ansi-red-fg\">         </span><span class=\"ansi-green-fg\">assert</span> isinstance<span class=\"ansi-blue-fg\">(</span>col<span class=\"ansi-blue-fg\">,</span> Column<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">&#34;col should be Column&#34;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   2478</span>         <span class=\"ansi-green-fg\">return</span> DataFrame<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_jdf<span class=\"ansi-blue-fg\">.</span>withColumn<span class=\"ansi-blue-fg\">(</span>colName<span class=\"ansi-blue-fg\">,</span> col<span class=\"ansi-blue-fg\">.</span>_jc<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> self<span class=\"ansi-blue-fg\">.</span>sql_ctx<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   2479</span> \n\n<span class=\"ansi-red-fg\">AssertionError</span>: col should be Column</div>","errorSummary":"<span class=\"ansi-red-fg\">AssertionError</span>: col should be Column","metadata":{},"errorTraceType":"html","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">AssertionError</span>                            Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-579747323490749&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      1</span> <span class=\"ansi-green-fg\">from</span> pyspark<span class=\"ansi-blue-fg\">.</span>sql<span class=\"ansi-blue-fg\">.</span>functions <span class=\"ansi-green-fg\">import</span> col\n<span class=\"ansi-green-fg\">----&gt; 2</span><span class=\"ansi-red-fg\"> </span>df<span class=\"ansi-blue-fg\">=</span> df111<span class=\"ansi-blue-fg\">.</span>withColumn<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;castedcol&#34;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">[</span>df111<span class=\"ansi-blue-fg\">.</span>dtypes<span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>show<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/dataframe.py</span> in <span class=\"ansi-cyan-fg\">withColumn</span><span class=\"ansi-blue-fg\">(self, colName, col)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   2475</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   2476</span>         &#34;&#34;&#34;\n<span class=\"ansi-green-fg\">-&gt; 2477</span><span class=\"ansi-red-fg\">         </span><span class=\"ansi-green-fg\">assert</span> isinstance<span class=\"ansi-blue-fg\">(</span>col<span class=\"ansi-blue-fg\">,</span> Column<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">&#34;col should be Column&#34;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   2478</span>         <span class=\"ansi-green-fg\">return</span> DataFrame<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_jdf<span class=\"ansi-blue-fg\">.</span>withColumn<span class=\"ansi-blue-fg\">(</span>colName<span class=\"ansi-blue-fg\">,</span> col<span class=\"ansi-blue-fg\">.</span>_jc<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> self<span class=\"ansi-blue-fg\">.</span>sql_ctx<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   2479</span> \n\n<span class=\"ansi-red-fg\">AssertionError</span>: col should be Column</div>"]},"transient":null}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Sample","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":3041300921854009}},"nbformat":4,"nbformat_minor":0}
